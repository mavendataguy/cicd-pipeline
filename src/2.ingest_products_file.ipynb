{"cells":[{"cell_type":"markdown","source":["### Ingest products.csv file\n##### The products file is meant for a dimension table\n##### We are following SCD - Type 1, whereby the product name will be updated in the dimension table every time there is a change in the source data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d086d17-8f9f-4c81-8473-1e56668cbad9"}}},{"cell_type":"markdown","source":["##### Let's pass on Data Source name as parameter - It can be passed from Azure Data Factory as well"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3eed64ab-ced5-452b-87e1-7dc9283a90e5"}}},{"cell_type":"code","source":["dbutils.widgets.text(\"p_data_source\", \"\")\nv_data_source = dbutils.widgets.get(\"p_data_source\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adcc84ef-d4a2-401b-a3db-0243dec5a36c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### Let's pass on Ingestion Date as parameter - It can be passed from Azure Data Factory as well"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f869283b-6cd7-4256-b1ab-0673dbaf3124"}}},{"cell_type":"code","source":["dbutils.widgets.text(\"p_file_date\", \"2021-03-28\")\nv_file_date = dbutils.widgets.get(\"p_file_date\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa39f920-e291-483d-8a93-cfa94ec3bbf6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The configuration_ecom.py file contains the folder paths of the data landing zone"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11dbf174-29a1-4f74-80b4-ae5a84acd52e"}}},{"cell_type":"code","source":["%run \"./configuration_ecom\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6e8a76b-a89f-48f7-8f15-788f471f85ae"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The common_function_ecom.py file contains various functions to handle the routine ETL matters, like adding an ingestion_date column to a dataframe, re-arranging the columns sequence if the columns are not in desired sequence, functions to check if a table exists or not, & last but not least the Merge function to handle insert/update at the same time ;)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e73235f0-df6c-4c49-a6e7-e00ec17f498c"}}},{"cell_type":"code","source":["%run \"./common_functions_ecom\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6a92d4e-af00-48b8-80d8-1bd4f98b440f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### Step 1 - Read the CSV file using the spark dataframe reader API"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c10a9b41-e54b-4be3-9579-680b36628fdb"}}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61fa87da-9c0b-4cad-be57-16705af9ca63"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["products_schema = StructType(fields=[StructField(\"product_Id\", IntegerType(), False),\n                                    StructField(\"created_at\", DateType(), True),\n                                    StructField(\"product_name\", StringType(), True)])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"059a5f92-5e4d-4bc0-a45c-26ce6fd8feeb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["results_df = spark.read \\\n.schema(products_schema).option(\"header\",True)\\\n.csv(f\"{raw_folder_path}/products/{v_file_date}/products.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd8e1ade-2c64-4846-922c-22aac59174ab"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### Step 2 - Rename columns and add new columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11a12637-558c-4342-8375-821d65060739"}}},{"cell_type":"code","source":["from pyspark.sql.functions import lit"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b91a57ac-2007-409d-bce9-342c80193202"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["results_with_columns_df = results_df.withColumn(\"data_source\", lit(v_data_source)) \\\n                                    .withColumn(\"ingestion_date\", lit(v_file_date))\n#results_with_columns_df = results_df.withColumnRenamed(\"created_at\", \"sourced_at\") \\"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50592917-97ec-4c67-b891-1027c955e066"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["results_with_ingestion_date_df = add_ingestion_date(results_with_columns_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c15794d7-06da-48ae-867d-b3a47f15585e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### Step 3 - Drop the unwanted columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70470f00-def4-4cf1-a883-1495b79bfa71"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38384750-83f9-4a39-8883-e65da4ca4465"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["results_final_df = results_with_ingestion_date_df.drop(col(\"data_source\"))\n#results_final_df.show(10,False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"36dedb37-ac08-45c1-8935-b8bbd450e9a8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["De-dupe the dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9370ebb5-30c2-4c7b-bd9e-7ee9fd84bfc8"}}},{"cell_type":"code","source":["results_deduped_df = results_final_df.dropDuplicates(['product_Id'])\n#results_deduped_df = results_final_df.dropDuplicates(['product_Id', 'created_at'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60385707-c3ef-4aa1-90f3-69ad18b10945"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### Step 4 - Write to Dataframe to designated Azure Data Lake container/directory in parquet format"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58b96c9c-291b-4f59-800c-c92f91554af4"}}},{"cell_type":"markdown","source":["The function 'merge_delta_data' will create a table it if does not exist"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34d9c19b-a066-4bdc-b8a7-05ce92438f31"}}},{"cell_type":"markdown","source":["##### Lets use Merge Function (merge_delta_data) using Python \nDatabricks DeltaLake supports ACID & Merge Function\nLet's pass on below arguments to Merge Function (located in common_functions_ecom.py)\n\n  Input DataFrame,  \n  \n  Database Name, \n  \n  Table Name, \n  \n  Folder Path (Target Table's path), \n  \n  Merge Condition \n  \n  and Partition Column (not using at the moment due to ir-relevance, \n  \n  <b>\nNote: products is a small dimension table & does not need partition. The dataframe cache feature will server the purpose to avoid expensive data shuffle."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1aea531-83e4-4d74-b23c-6341688fa369"}}},{"cell_type":"code","source":["merge_condition = \"tgt.product_Id = src.product_Id\"\nmerge_delta_data(results_deduped_df, 'ecom_growth', 'products', processed_folder_path, merge_condition, 'product_Id')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46cac9cd-2701-4246-b506-c51a929be46c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.notebook.exit(\"Success\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b352a725-a812-4941-8aec-3d6f3c631b63"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"exit","data":"Success","arguments":{},"metadata":{}}},"output_type":"display_data","data":{"text/plain":["Success"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT count(*)\n  FROM ecom_growth.products;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf65d547-43e7-404e-9891-8c714194b982"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[4]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"count(1)","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>4</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<b> SQL version of Merge Statement\n\nNote:  we have to register a Temporary View named 'temp_view' from DataFrame results_deduped_df\n  \n%sql\n  \nMERGE INTO ecom_growth.products\n\nUSING temp_view\n\nON ecom_growth.products.product_Id = temp_view.product_Id\n\nWHEN MATCHED THEN\n\n  UPDATE SET ecom_growth.products.product_name = temp_view.product_name\n  \nWHEN NOT MATCHED\n\n  THEN INSERT (product_id,created_at,product_name,ingestion_date) VALUES (product_id,created_at,product_name,ingestion_date)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3ed029a-adae-4eda-b27a-e7406f9a4a6e"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b4aa38b-b742-4717-acf1-6afadb229d56"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2.ingest_products_file","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{"p_data_source":{"nuid":"6ae13916-9f47-4659-88eb-b4196b247985","currentValue":"None","widgetInfo":{"widgetType":"text","name":"p_data_source","defaultValue":"","label":null,"options":{"widgetType":"text","validationRegex":null}}},"p_file_date":{"nuid":"bb7aaf1a-800d-457a-811d-7999eb74a28c","currentValue":"2021-12-15","widgetInfo":{"widgetType":"text","name":"p_file_date","defaultValue":"2021-03-28","label":null,"options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":3480039094636339}},"nbformat":4,"nbformat_minor":0}
